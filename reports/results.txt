
(tf) dhvanishah@Dhvanis-MBP vgg3 % python train_pipeline.py
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.
Train: X=(50000, 32, 32, 3), y=(50000, 1)
Test: X=(10000, 32, 32, 3), y=(10000, 1)
/Users/dhvanishah/opt/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
Epoch 1/200
  1/834 [..............................] - ETA: 5:36 - loss: 3.7082 - accuracy: 0.10      3/834 [..............................] - ETA: 32s - loss: 3.0687 - accuracy: 0.090      5/834 [..............................] - ETA: 33s - loss: 2.8721 - accuracy: 0.100      7/834 [..............................] - ETA: 32s - loss: 2.8029 - accuracy: 0.089      9/834 [..............................] - ETA: 32s - loss: 2.7187 - accuracy: 0.094     11/834 [..............................] - ETA: 32s - loss: 2.6627 - accuracy: 0.098     13/834 [..............................] - ETA: 31s - loss: 2.6177 - accuracy: 0.099     15/834 [..............................] - ETA: 31s - loss: 2.5834 - accuracy: 0.108     17/834 [..............................] - ETA: 31s - loss: 2.5521 - accuracy: 0.110     19/834 [..............................] - ETA: 31s - loss: 2.5263 - accuracy: 0.110     21/834 [..............................] - ETA: 31s - loss: 2.5080 - accuracy: 0.110     23/834 [..............................] - ETA: 30s - loss: 2.4920 - accuracy: 0.110     25/834 [..............................] - ETA: 30s - loss: 2.4774 - accuracy: 0.112     27/834 [..............................] - ETA: 30s - loss: 2.4664 - accuracy: 0.113     28/834 [>.............................] - ETA: 31s - loss: 2.4615 - accuracy: 0.113     30/834 [>.............................] - ETA: 31s - loss: 2.4530 - accuracy: 0.116     32/834 [>.............................] - ETA: 31s - loss: 2.4431 - accuracy: 0.115     34/834 [>.............................] - ETA: 30s - loss: 2.4342 - accuracy: 0.114     36/834 [>.............................] - ETA: 30s - loss: 2.4266 - accuracy: 0.116     38/834 [>.............................] - ETA: 30s - loss: 2.4195 - accuracy: 0.118     40/834 [>.............................] - ETA: 30s - loss: 2.4130 - accuracy: 0.121     42/834 [>.............................] - ETA: 30s - loss: 2.4076 - accuracy: 0.122     44/834 [>.............................] - ETA: 30s - loss: 2.4023 - accuracy: 0.123     46/834 [>.............................] - ETA: 29s - loss: 2.3966 - accuracy: 0.124     48/834 [>.............................] - ETA: 29s - loss: 2.3930 - accuracy: 0.124   50/834 [>.............................] - ETA: 29s - loss: 2.3888 - accuracy: 0.123     52/834 [>.............................] - ETA: 29s - loss: 2.3841 - accuracy: 0.126     54/834 [>.............................] - ETA: 29s - loss: 2.3805 - accuracy: 0.125     56/834 [=>............................] - ETA: 29s - loss: 2.3774 - accuracy: 0.126     58/834 [=>............................] - ETA: 29s - loss: 2.3741 - accuracy: 0.124     60/834 [=>............................] - ETA: 29s - loss: 2.3700 - accuracy: 0.125     62/834 [=>............................] - ETA: 29s - loss: 2.3678 - accuracy: 0.124     64/834 [=>............................] - ETA: 29s - loss: 2.3651 - accuracy: 0.125     66/834 [=>............................] - ETA: 29s - loss: 2.3619 - accuracy: 0.126     68/834 [=>............................] - ETA: 29s - loss: 2.3596 - accuracy: 0.127     70/834 [=>............................] - ETA: 28s - loss: 2.3573 - accuracy: 0.126     72/834 [=>............................] - ETA: 28s - loss: 2.3552 - accuracy: 0.126     74/834 [=>............................] - ETA: 28s - loss: 2.3533 - accuracy: 0.126     76/834 [=>............................] - ETA: 28s - loss: 2.3510 - accuracy: 0.126     78/834 [=>............................] - ETA: 28s - loss: 2.3480 - accuracy: 0.128     80/834 [=>............................] - ETA: 28s - loss: 2.3457 - accuracy: 0.128     82/834 [=>............................] - ETA: 28s - loss: 2.3427 - accuracy: 0.129     84/834 [==>...........................] - ETA: 28s - loss: 2.3404 - accuracy: 0.129     86/834 [==>...........................] - ETA: 28s - loss: 2.3388 - accuracy: 0.129     88/834 [==>...........................] - ETA: 28s - loss: 2.3364 - accuracy: 0.130     90/834 [==>...........................] - ETA: 28s - loss: 2.3347 - accuracy: 0.130     92/834 [==>...........................] - ETA: 28s - loss: 2.3329 - accuracy: 0.130     94/834 [==>...........................] - ETA: 27s - loss: 2.3309 - accuracy: 0.131     96/834 [==>...........................] - ETA: 27s - loss: 2.3288 - accuracy: 0.131     98/834 [==>...........................] - ETA: 27s - loss: 2.3267 - accuracy: 0.132    100/834 [==>...........................] - ETA: 27s - loss: 2.3244 - accuracy: 0.132    102/834 [==>...........................] - ETA: 27s - loss: 2.3219 - accuracy: 0.133    104/834 [==>...........................] - ETA: 27s - loss: 2.3196 - accuracy: 0.134    106/834 [==>...........................] - ETA: 27s - loss: 2.3177 - accuracy: 0.134    108/834 [==>...........................] - ETA: 27s - loss: 2.3158 - accuracy: 0.134    110/834 [==>...........................] - ETA: 27s - loss: 2.3141 - accuracy: 0.135    112/834 [===>..........................] - ETA: 27s - loss: 2.3114 - accuracy: 0.136    114/834 [===>..........................] - ETA: 27s - loss: 2.3096 - accuracy: 0.137    116/834 [===>..........................] - ETA: 27s - loss: 2.3076 - accuracy: 0.138    118/834 [===>..........................] - ETA: 27s - loss: 2.3055 - accuracy: 0.138    120/834 [===>..........................] - ETA: 27s - loss: 2.3033 - accuracy: 0.139    122/834 [===>..........................] - ETA: 26s - loss: 2.3016 - accuracy: 0.138    124/834 [===>..........................] - ETA: 26s - loss: 2.2986 - accuracy: 0.139    126/834 [===>..........................] - ETA: 26s - loss: 2.2967 - accuracy: 0.140    128/834 [===>..........................] - ETA: 26s - loss: 2.2963 - accuracy: 0.140    130/834 [===>..........................] - ETA: 26s - loss: 2.2946 - accuracy: 0.140    132/834 [===>..........................] - ETA: 26s - loss: 2.2920 - accuracy: 0.141    134/834 [===>..........................] - ETA: 26s - loss: 2.2900 - accuracy: 0.142    136/834 [===>..........................] - ETA: 26s - loss: 2.2885 - accuracy: 0.142    138/834 [===>..........................] - ETA: 26s - loss: 2.2869 - accuracy: 0.143    140/834 [====>.........................] - ETA: 26s - loss: 2.2843 - accuracy: 0.144    142/834 [====>.........................] - ETA: 26s - loss: 2.2819 - accuracy: 0.145    144/834 [====>.........................] - ETA: 26s - loss: 2.2800 - accuracy: 0.146    146/834 [====>.........................] - ETA: 26s - loss: 2.2766 - accuracy: 0.147    148/834 [====>.........................] - ETA: 25s - loss: 2.2738 - accuracy: 0.149    150/834 [====>.........................] - ETA: 25s - loss: 2.2714 - accuracy: 0.150    152/834 [====>.........................] - ETA: 25s - loss: 2.2695 - accuracy: 0.151    154/834 [====>.........................] - ETA: 25s - loss: 2.2682 - accuracy: 0.151    156/834 [====>.........................] - ETA: 25s - loss: 2.2664 - accuracy: 0.151    158/834 [====>.........................] - ETA: 25s - loss: 2.2645 - accuracy: 0.152    160/834 [====>.........................] - ETA: 25s - loss: 2.2632 - accuracy: 0.153    162/834 [====>.........................] - ETA: 25s - loss: 2.2620 - accuracy: 0.153    164/834 [====>.........................] - ETA: 25s - loss: 2.2590 - accuracy: 0.154    166/834 [====>.........................] - ETA: 25s - loss: 2.2557 - accuracy: 0.156    167/834 [=====>........................] - ETA: 25s - loss: 2.2551 - accuracy: 0.156    169/834 [=====>........................] - ETA: 25s - loss: 2.2532 - accuracy: 0.156    171/834 [=====>........................] - ETA: 25s - loss: 2.2519 - accuracy: 0.157    173/834 [=====>........................] - ETA: 25s - loss: 2.2487 - accuracy: 0.159    175/834 [=====>........................] - ETA: 25s - loss: 2.2464 - accuracy: 0.159    177/834 [=====>........................] - ETA: 24s - loss: 2.2445 - accuracy: 0.160    179/834 [=====>........................] - ETA: 24s - loss: 2.2427 - accuracy: 0.161 181/834 [=====>........................] - ETA: 24s - loss: 2.2403 - accuracy: 0.161    183/834 [=====>........................] - ETA: 24s - loss: 2.2387 - accuracy: 0.162    185/834 [=====>........................] - ETA: 24s - loss: 2.2366 - accuracy: 0.163    187/834 [=====>........................] - ETA: 24s - loss: 2.2347 - accuracy: 0.164    189/834 [=====>........................] - ETA: 24s - loss: 2.2338 - accuracy: 0.164    191/834 [=====>........................] - ETA: 24s - loss: 2.2317 - accuracy: 0.165    193/834 [=====>........................] - ETA: 24s - loss: 2.2305 - accuracy: 0.166    195/834 [======>.......................] - ETA: 24s - loss: 2.2279 - accuracy: 0.167    197/834 [======>.......................] - ETA: 24s - loss: 2.2256 - accuracy: 0.167    199/834 [======>.......................] - ETA: 24s - loss: 2.2238 - accuracy: 0.168    201/834 [======>.......................] - ETA: 24s - loss: 2.2218 - accuracy: 0.168    203/834 [======>.......................] - ETA: 23s - loss: 2.2202 - accuracy: 0.170    205/834 [======>.......................] - ETA: 23s - loss: 2.2191 - accuracy: 0.170    207/834 [======>.......................] - ETA: 23s - loss: 2.2178 - accuracy: 0.170    209/834 [======>.......................] - ETA: 23s - loss: 2.2165 - accuracy: 0.171    211/834 [======>.......................] - ETA: 23s - loss: 2.2158 - accuracy: 0.171    213/834 [======>.......................] - ETA: 23s - loss: 2.2144 - accuracy: 0.172    215/834 [======>.......................] - ETA: 23s - loss: 2.2136 - accuracy: 0.173    217/834 [======>.......................] - ETA: 23s - loss: 2.2109 - accuracy: 0.174    219/834 [======>.......................] - ETA: 23s - loss: 2.2087 - accuracy: 0.175    221/834 [======>.......................] - ETA: 23s - loss: 2.2070 - accuracy: 0.175    223/834 [=======>......................] - ETA: 23s - loss: 2.2045 - accuracy: 0.176    225/834 [=======>......................] - ETA: 23s - loss: 2.2033 - accuracy: 0.176    227/834 [=======>......................] - ETA: 23s - loss: 2.2019 - accuracy: 0.176    229/834 [=======>......................] - ETA: 22s - loss: 2.2003 - accuracy: 0.177   231/834 [=======>......................] - ETA: 22s - loss: 2.1980 - accuracy: 0.178    233/834 [=======>......................] - ETA: 22s - loss: 2.1961 - accuracy: 0.178    235/834 [=======>......................] - ETA: 22s - loss: 2.1941 - accuracy: 0.179    237/834 [=======>......................] - ETA: 22s - loss: 2.1931 - accuracy: 0.179    239/834 [=======>......................] - ETA: 22s - loss: 2.1927 - accuracy: 0.179    241/834 [=======>......................] - ETA: 22s - loss: 2.1909 - accuracy: 0.180    243/834 [=======>......................] - ETA: 22s - loss: 2.1886 - accuracy: 0.180    245/834 [=======>......................] - ETA: 22s - loss: 2.1878 - accuracy: 0.181    247/834 [=======>......................] - ETA: 22s - loss: 2.1864 - accuracy: 0.182    249/834 [=======>......................] - ETA: 22s - loss: 2.1852 - accuracy: 0.182    251/834 [========>.....................] - ETA: 22s - loss: 2.1841 - accuracy: 0.183    253/834 [========>.....................] - ETA: 21s - loss: 2.1827 - accuracy: 0.183    255/834 [========>.....................] - ETA: 21s - loss: 2.1820 - accuracy: 0.184    257/834 [========>.....................] - ETA: 21s - loss: 2.1806 - accuracy: 0.184    259/834 [========>.....................] - ETA: 21s - loss: 2.1796 - accuracy: 0.185    261/834 [========>.....................] - ETA: 21s - loss: 2.1780 - accuracy: 0.185    263/834 [========>.....................] - ETA: 21s - loss: 2.1769 - accuracy: 0.186    265/834 [========>.....................] - ETA: 21s - loss: 2.1758 - accuracy: 0.187    267/834 [========>.....................] - ETA: 21s - loss: 2.1753 - accuracy: 0.187    269/834 [========>.....................] - ETA: 21s - loss: 2.1735 - accuracy: 0.187    271/834 [========>.....................] - ETA: 21s - loss: 2.1725 - accuracy: 0.187    273/834 [========>.....................] - ETA: 21s - loss: 2.1712 - accuracy: 0.187    275/834 [========>.....................] - ETA: 21s - loss: 2.1713 - accuracy: 0.188    277/834 [========>.....................] - ETA: 21s - loss: 2.1704 - accuracy: 0.188    279/834 [=========>....................] - ETA: 20s - loss: 2.1688 - accuracy: 0.189    281/834 [=========>....................] - ETA: 20s - loss: 2.1683 - accuracy: 0.189    283/834 [=========>....................] - ETA: 20s - loss: 2.1676 - accuracy: 0.190    285/834 [=========>....................] - ETA: 20s - loss: 2.1665 - accuracy: 0.190    287/834 [=========>....................] - ETA: 20s - loss: 2.1655 - accuracy: 0.191    289/834 [=========>....................] - ETA: 20s - loss: 2.1638 - accuracy: 0.192    291/834 [=========>....................] - ETA: 20s - loss: 2.1628 - accuracy: 0.192    293/834 [=========>....................] - ETA: 20s - loss: 2.1619 - accuracy: 0.192    295/834 [=========>....................] - ETA: 20s - loss: 2.1607 - accuracy: 0.193    297/834 [=========>....................] - ETA: 20s - loss: 2.1598 - accuracy: 0.193    299/834 [=========>....................] - ETA: 20s - loss: 2.1586 - accuracy: 0.193    301/834 [=========>....................] - ETA: 20s - loss: 2.1578 - accuracy: 0.194    303/834 [=========>....................] - ETA: 20s - loss: 2.1564 - accuracy: 0.195    305/834 [=========>....................] - ETA: 19s - loss: 2.1553 - accuracy: 0.195    307/834 [==========>...................] - ETA: 19s - loss: 2.1536 - accuracy: 0.196    309/834 [==========>...................] - ETA: 19s - loss: 2.1525 - accuracy: 0.196    311/834 [==========>...................] - ETA: 19s - loss: 2.1510 - accuracy: 0.197313/834 [==========>...................] - ETA: 19s - loss: 2.1501 - accuracy: 0.197    315/834 [==========>...................] - ETA: 19s - loss: 2.1490 - accuracy: 0.197    317/834 [==========>...................] - ETA: 19s - loss: 2.1479 - accuracy: 0.198    319/834 [==========>...................] - ETA: 19s - loss: 2.1466 - accuracy: 0.198    321/834 [==========>...................] - ETA: 19s - loss: 2.1457 - accuracy: 0.198    322/834 [==========>...................] - ETA: 19s - loss: 2.1445 - accuracy: 0.199    323/834 [==========>...................] - ETA: 19s - loss: 2.1439 - accuracy: 0.199    325/834 [==========>...................] - ETA: 19s - loss: 2.1424 - accuracy: 0.200    327/834 [==========>...................] - ETA: 19s - loss: 2.1418 - accuracy: 0.200    329/834 [==========>...................] - ETA: 19s - loss: 2.1400 - accuracy: 0.201    331/834 [==========>...................] - ETA: 19s - loss: 2.1383 - accuracy: 0.202    333/834 [==========>...................] - ETA: 19s - loss: 2.1373 - accuracy: 0.202    335/834 [===========>..................] - ETA: 19s - loss: 2.1360 - accuracy: 0.202    337/834 [===========>..................] - ETA: 18s - loss: 2.1350 - accuracy: 0.202    339/834 [===========>..................] - ETA: 18s - loss: 2.1339 - accuracy: 0.203    341/834 [===========>..................] - ETA: 18s - loss: 2.1326 - accuracy: 0.203    343/834 [===========>..................] - ETA: 18s - loss: 2.1309 - accuracy: 0.204    344/834 [===========>..................] - ETA: 18s - loss: 2.1305 - accuracy: 0.204    345/834 [===========>..................] - ETA: 18s - loss: 2.1296 - accuracy: 0.205    346/834 [===========>..................] - ETA: 18s - loss: 2.1285 - accuracy: 0.205    348/834 [===========>..................] - ETA: 18s - loss: 2.1267 - accuracy: 0.205    350/834 [===========>..................] - ETA: 18s - loss: 2.1256 - accuracy: 0.206    352/834 [===========>..................] - ETA: 18s - loss: 2.1241 - accuracy: 0.207    354/834 [===========>..................] - ETA: 18s - loss: 2.1229 - accuracy: 0.207    356/834 [===========>..................] - ETA: 18s - loss: 2.1221 - accuracy: 0.207  358/834 [===========>..................] - ETA: 18s - loss: 2.1214 - accuracy: 0.208    360/834 [===========>..................] - ETA: 18s - loss: 2.1196 - accuracy: 0.208    362/834 [============>.................] - ETA: 18s - loss: 2.1184 - accuracy: 0.209    364/834 [============>.................] - ETA: 18s - loss: 2.1186 - accuracy: 0.209    366/834 [============>.................] - ETA: 17s - loss: 2.1177 - accuracy: 0.209    368/834 [============>.................] - ETA: 17s - loss: 2.1168 - accuracy: 0.210    370/834 [============>.................] - ETA: 17s - loss: 2.1158 - accuracy: 0.210    372/834 [============>.................] - ETA: 17s - loss: 2.1154 - accuracy: 0.210    374/834 [============>.................] - ETA: 17s - loss: 2.1144 - accuracy: 0.210    376/834 [============>.................] - ETA: 17s - loss: 2.1129 - accuracy: 0.212    378/834 [============>.................] - ETA: 17s - loss: 2.1121 - accuracy: 0.212    380/834 [============>.................] - ETA: 17s - loss: 2.1113 - accuracy: 0.212    382/834 [============>.................] - ETA: 17s - loss: 2.1100 - accuracy: 0.213    384/834 [============>.................] - ETA: 17s - loss: 2.1093 - accuracy: 0.213    386/834 [============>.................] - ETA: 17s - loss: 2.1084 - accuracy: 0.213    388/834 [============>.................] - ETA: 17s - loss: 2.1070 - accuracy: 0.214    390/834 [=============>................] - ETA: 17s - loss: 2.1055 - accuracy: 0.215    392/834 [=============>................] - ETA: 16s - loss: 2.1053 - accuracy: 0.215    394/834 [=============>................] - ETA: 16s - loss: 2.1044 - accuracy: 0.215    396/834 [=============>................] - ETA: 16s - loss: 2.1031 - accuracy: 0.216    398/834 [=============>................] - ETA: 16s - loss: 2.1025 - accuracy: 0.216    400/834 [=============>................] - ETA: 16s - loss: 2.1019 - accuracy: 0.217    402/834 [=============>................] - ETA: 16s - loss: 2.1008 - accuracy: 0.217    404/834 [=============>................] - ETA: 16s - loss: 2.0994 - accuracy: 0.218    406/834 [=============>................] - ETA: 16s - loss: 2.0989 - accuracy: 0.218    408/834 [=============>................] - ETA: 16s - loss: 2.0985 - accuracy: 0.218    410/834 [=============>................] - ETA: 16s - loss: 2.0980 - accuracy: 0.218    412/834 [=============>................] - ETA: 16s - loss: 2.0971 - accuracy: 0.218    414/834 [=============>................] - ETA: 16s - loss: 2.0956 - accuracy: 0.218    416/834 [=============>................] - ETA: 16s - loss: 2.0953 - accuracy: 0.218    418/834 [==============>...............] - ETA: 15s - loss: 2.0938 - accuracy: 0.219    420/834 [==============>...............] - ETA: 15s - loss: 2.0926 - accuracy: 0.219    422/834 [==============>...............] - ETA: 15s - loss: 2.0920 - accuracy: 0.220    424/834 [==============>...............] - ETA: 15s - loss: 2.0907 - accuracy: 0.220    426/834 [==============>...............] - ETA: 15s - loss: 2.0901 - accuracy: 0.220    427/834 [==============>...............] - ETA: 15s - loss: 2.0893 - accuracy: 0.220    428/834 [==============>...............] - ETA: 15s - loss: 2.0887 - accuracy: 0.221    429/834 [==============>...............] - ETA: 15s - loss: 2.0881 - accuracy: 0.221    431/834 [==============>...............] - ETA: 15s - loss: 2.0866 - accuracy: 0.222    433/834 [==============>...............] - ETA: 15s - loss: 2.0860 - accuracy: 0.222    435/834 [==============>...............] - ETA: 15s - loss: 2.0854 - accuracy: 0.222    437/834 [==============>...............] - ETA: 15s - loss: 2.0849 - accuracy: 0.222    834/834 [==============================] - ETA: 0s - loss: 1.9440 - accuracy: 0.2798 
Epoch 1: val_accuracy improved from -inf to 0.39150, saving model to artifacts/cnn_model.h5
/Users/dhvanishah/opt/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
834/834 [==============================] - 35s 42ms/step - loss: 1.9440 - accuracy: 0.2798 - val_loss: 1.6684 - val_accuracy: 0.3915 - lr: 0.0010
Epoch 2/200
833/834 [============================>.] - ETA: 0s - loss: 1.6270 - accuracy: 0.3981 
Epoch 2: val_accuracy improved from 0.39150 to 0.45810, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 1.6271 - accuracy: 0.3981 - val_loss: 1.4836 - val_accuracy: 0.4581 - lr: 0.0010
Epoch 3/200
834/834 [==============================] - ETA: 0s - loss: 1.4822 - accuracy: 0.4586 
Epoch 3: val_accuracy improved from 0.45810 to 0.52900, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 1.4822 - accuracy: 0.4586 - val_loss: 1.3472 - val_accuracy: 0.5290 - lr: 0.0010
Epoch 4/200
833/834 [============================>.] - ETA: 0s - loss: 1.3905 - accuracy: 0.4930 
Epoch 4: val_accuracy improved from 0.52900 to 0.55040, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 41ms/step - loss: 1.3904 - accuracy: 0.4930 - val_loss: 1.2749 - val_accuracy: 0.5504 - lr: 0.0010
Epoch 5/200
834/834 [==============================] - ETA: 0s - loss: 1.2894 - accuracy: 0.5355 
Epoch 5: val_accuracy improved from 0.55040 to 0.57140, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 1.2894 - accuracy: 0.5355 - val_loss: 1.2004 - val_accuracy: 0.5714 - lr: 0.0010
Epoch 6/200
833/834 [============================>.] - ETA: 0s - loss: 1.2186 - accuracy: 0.5629 
Epoch 6: val_accuracy improved from 0.57140 to 0.62230, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 1.2185 - accuracy: 0.5628 - val_loss: 1.0737 - val_accuracy: 0.6223 - lr: 0.0010
Epoch 7/200
833/834 [============================>.] - ETA: 0s - loss: 1.1558 - accuracy: 0.5877 
Epoch 7: val_accuracy improved from 0.62230 to 0.62300, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 1.1559 - accuracy: 0.5877 - val_loss: 1.0776 - val_accuracy: 0.6230 - lr: 0.0010
Epoch 8/200
834/834 [==============================] - ETA: 0s - loss: 1.1050 - accuracy: 0.6061 
Epoch 8: val_accuracy improved from 0.62300 to 0.65390, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 1.1050 - accuracy: 0.6061 - val_loss: 0.9891 - val_accuracy: 0.6539 - lr: 0.0010
Epoch 9/200
834/834 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.6223 
Epoch 9: val_accuracy improved from 0.65390 to 0.66480, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 1.0610 - accuracy: 0.6223 - val_loss: 0.9570 - val_accuracy: 0.6648 - lr: 0.0010
Epoch 10/200
833/834 [============================>.] - ETA: 0s - loss: 1.0127 - accuracy: 0.6391 
Epoch 10: val_accuracy improved from 0.66480 to 0.67230, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 1.0126 - accuracy: 0.6391 - val_loss: 0.9368 - val_accuracy: 0.6723 - lr: 0.0010
Epoch 11/200
834/834 [==============================] - ETA: 0s - loss: 0.9804 - accuracy: 0.6506 
Epoch 11: val_accuracy improved from 0.67230 to 0.68970, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.9804 - accuracy: 0.6506 - val_loss: 0.8897 - val_accuracy: 0.6897 - lr: 0.0010
Epoch 12/200
834/834 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.6635 
Epoch 12: val_accuracy did not improve from 0.68970
834/834 [==============================] - 35s 42ms/step - loss: 0.9516 - accuracy: 0.6635 - val_loss: 0.8878 - val_accuracy: 0.6859 - lr: 0.0010
Epoch 13/200
833/834 [============================>.] - ETA: 0s - loss: 0.9128 - accuracy: 0.6734 
Epoch 13: val_accuracy improved from 0.68970 to 0.70910, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.9128 - accuracy: 0.6734 - val_loss: 0.8290 - val_accuracy: 0.7091 - lr: 0.0010
Epoch 14/200
834/834 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.6844 
Epoch 14: val_accuracy improved from 0.70910 to 0.70940, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.8883 - accuracy: 0.6844 - val_loss: 0.8282 - val_accuracy: 0.7094 - lr: 0.0010
Epoch 15/200
834/834 [==============================] - ETA: 0s - loss: 0.8678 - accuracy: 0.6930 
Epoch 15: val_accuracy improved from 0.70940 to 0.72200, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.8678 - accuracy: 0.6930 - val_loss: 0.7864 - val_accuracy: 0.7220 - lr: 0.0010
Epoch 16/200
834/834 [==============================] - ETA: 0s - loss: 0.8381 - accuracy: 0.7035 
Epoch 16: val_accuracy improved from 0.72200 to 0.72590, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.8381 - accuracy: 0.7035 - val_loss: 0.7812 - val_accuracy: 0.7259 - lr: 0.0010
Epoch 17/200
833/834 [============================>.] - ETA: 0s - loss: 0.8218 - accuracy: 0.7108 
Epoch 17: val_accuracy did not improve from 0.72590
834/834 [==============================] - 36s 43ms/step - loss: 0.8218 - accuracy: 0.7107 - val_loss: 0.7896 - val_accuracy: 0.7222 - lr: 0.0010
Epoch 18/200
834/834 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.7166 
Epoch 18: val_accuracy improved from 0.72590 to 0.73760, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.8014 - accuracy: 0.7166 - val_loss: 0.7441 - val_accuracy: 0.7376 - lr: 0.0010
Epoch 19/200
834/834 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.7262  
Epoch 19: val_accuracy improved from 0.73760 to 0.73790, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 37s 44ms/step - loss: 0.7736 - accuracy: 0.7262 - val_loss: 0.7477 - val_accuracy: 0.7379 - lr: 0.0010
Epoch 20/200
833/834 [============================>.] - ETA: 0s - loss: 0.7626 - accuracy: 0.7316 
Epoch 20: val_accuracy improved from 0.73790 to 0.74520, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.7626 - accuracy: 0.7316 - val_loss: 0.7294 - val_accuracy: 0.7452 - lr: 0.0010
Epoch 21/200
834/834 [==============================] - ETA: 0s - loss: 0.7425 - accuracy: 0.7373 
Epoch 21: val_accuracy improved from 0.74520 to 0.75740, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.7425 - accuracy: 0.7373 - val_loss: 0.6929 - val_accuracy: 0.7574 - lr: 0.0010
Epoch 22/200
834/834 [==============================] - ETA: 0s - loss: 0.7314 - accuracy: 0.7414 
Epoch 22: val_accuracy did not improve from 0.75740
834/834 [==============================] - 36s 43ms/step - loss: 0.7314 - accuracy: 0.7414 - val_loss: 0.7279 - val_accuracy: 0.7468 - lr: 0.0010
Epoch 23/200
833/834 [============================>.] - ETA: 0s - loss: 0.7026 - accuracy: 0.7531 
Epoch 23: val_accuracy improved from 0.75740 to 0.76200, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 37s 44ms/step - loss: 0.7025 - accuracy: 0.7531 - val_loss: 0.6831 - val_accuracy: 0.7620 - lr: 0.0010
Epoch 24/200
834/834 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.7541 
Epoch 24: val_accuracy did not improve from 0.76200
834/834 [==============================] - 36s 43ms/step - loss: 0.6942 - accuracy: 0.7541 - val_loss: 0.7025 - val_accuracy: 0.7535 - lr: 0.0010
Epoch 25/200
833/834 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.7580 
Epoch 25: val_accuracy improved from 0.76200 to 0.76790, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.6849 - accuracy: 0.7580 - val_loss: 0.6688 - val_accuracy: 0.7679 - lr: 0.0010
Epoch 26/200
834/834 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.7641 
Epoch 26: val_accuracy did not improve from 0.76790
834/834 [==============================] - 34s 41ms/step - loss: 0.6708 - accuracy: 0.7641 - val_loss: 0.6771 - val_accuracy: 0.7628 - lr: 0.0010
Epoch 27/200
833/834 [============================>.] - ETA: 0s - loss: 0.6469 - accuracy: 0.7720 
Epoch 27: val_accuracy improved from 0.76790 to 0.76940, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.6469 - accuracy: 0.7720 - val_loss: 0.6685 - val_accuracy: 0.7694 - lr: 0.0010
Epoch 28/200
833/834 [============================>.] - ETA: 0s - loss: 0.6428 - accuracy: 0.7728 
Epoch 28: val_accuracy did not improve from 0.76940
834/834 [==============================] - 36s 44ms/step - loss: 0.6429 - accuracy: 0.7728 - val_loss: 0.6850 - val_accuracy: 0.7625 - lr: 0.0010
Epoch 29/200
834/834 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7789  
Epoch 29: val_accuracy improved from 0.76940 to 0.77130, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.6293 - accuracy: 0.7789 - val_loss: 0.6608 - val_accuracy: 0.7713 - lr: 0.0010
Epoch 30/200
834/834 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.7829 
Epoch 30: val_accuracy improved from 0.77130 to 0.78470, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.6110 - accuracy: 0.7829 - val_loss: 0.6225 - val_accuracy: 0.7847 - lr: 0.0010
Epoch 31/200
833/834 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7863 
Epoch 31: val_accuracy did not improve from 0.78470
834/834 [==============================] - 35s 42ms/step - loss: 0.6015 - accuracy: 0.7863 - val_loss: 0.6256 - val_accuracy: 0.7816 - lr: 0.0010
Epoch 32/200
833/834 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.7921 
Epoch 32: val_accuracy did not improve from 0.78470
834/834 [==============================] - 35s 41ms/step - loss: 0.5897 - accuracy: 0.7920 - val_loss: 0.6425 - val_accuracy: 0.7755 - lr: 0.0010
Epoch 33/200
833/834 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7966 
Epoch 33: val_accuracy did not improve from 0.78470
834/834 [==============================] - 35s 42ms/step - loss: 0.5781 - accuracy: 0.7967 - val_loss: 0.6409 - val_accuracy: 0.7798 - lr: 0.0010
Epoch 34/200
833/834 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7978 
Epoch 34: val_accuracy improved from 0.78470 to 0.78480, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.5699 - accuracy: 0.7979 - val_loss: 0.6145 - val_accuracy: 0.7848 - lr: 0.0010
Epoch 35/200
834/834 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8004 
Epoch 35: val_accuracy improved from 0.78480 to 0.78810, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.5619 - accuracy: 0.8004 - val_loss: 0.6080 - val_accuracy: 0.7881 - lr: 0.0010
Epoch 36/200
833/834 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.8076 
Epoch 36: val_accuracy improved from 0.78810 to 0.79410, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 35s 42ms/step - loss: 0.5454 - accuracy: 0.8077 - val_loss: 0.5895 - val_accuracy: 0.7941 - lr: 0.0010
Epoch 37/200
833/834 [============================>.] - ETA: 0s - loss: 0.5388 - accuracy: 0.8086 
Epoch 37: val_accuracy did not improve from 0.79410
834/834 [==============================] - 35s 42ms/step - loss: 0.5387 - accuracy: 0.8087 - val_loss: 0.5984 - val_accuracy: 0.7924 - lr: 0.0010
Epoch 38/200
833/834 [============================>.] - ETA: 0s - loss: 0.5285 - accuracy: 0.8121 
Epoch 38: val_accuracy improved from 0.79410 to 0.79510, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.5285 - accuracy: 0.8121 - val_loss: 0.5989 - val_accuracy: 0.7951 - lr: 0.0010
Epoch 39/200
833/834 [============================>.] - ETA: 0s - loss: 0.5147 - accuracy: 0.8172 
Epoch 39: val_accuracy did not improve from 0.79510
834/834 [==============================] - 34s 40ms/step - loss: 0.5149 - accuracy: 0.8172 - val_loss: 0.5947 - val_accuracy: 0.7929 - lr: 0.0010
Epoch 40/200
833/834 [============================>.] - ETA: 0s - loss: 0.5081 - accuracy: 0.8200 
Epoch 40: val_accuracy improved from 0.79510 to 0.80140, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 40ms/step - loss: 0.5082 - accuracy: 0.8199 - val_loss: 0.5845 - val_accuracy: 0.8014 - lr: 0.0010
Epoch 41/200
833/834 [============================>.] - ETA: 0s - loss: 0.4973 - accuracy: 0.8227 
Epoch 41: val_accuracy did not improve from 0.80140
834/834 [==============================] - 34s 40ms/step - loss: 0.4974 - accuracy: 0.8227 - val_loss: 0.6072 - val_accuracy: 0.7916 - lr: 0.0010
Epoch 42/200
833/834 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8283 
Epoch 42: val_accuracy did not improve from 0.80140
834/834 [==============================] - 34s 41ms/step - loss: 0.4842 - accuracy: 0.8282 - val_loss: 0.5900 - val_accuracy: 0.8007 - lr: 0.0010
Epoch 43/200
834/834 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8288 
Epoch 43: val_accuracy improved from 0.80140 to 0.80700, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.4808 - accuracy: 0.8288 - val_loss: 0.5692 - val_accuracy: 0.8070 - lr: 0.0010
Epoch 44/200
834/834 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.8301 
Epoch 44: val_accuracy did not improve from 0.80700
834/834 [==============================] - 34s 41ms/step - loss: 0.4745 - accuracy: 0.8301 - val_loss: 0.6008 - val_accuracy: 0.7973 - lr: 0.0010
Epoch 45/200
833/834 [============================>.] - ETA: 0s - loss: 0.4640 - accuracy: 0.8348 
Epoch 45: val_accuracy did not improve from 0.80700
834/834 [==============================] - 34s 41ms/step - loss: 0.4638 - accuracy: 0.8349 - val_loss: 0.5837 - val_accuracy: 0.8039 - lr: 0.0010
Epoch 46/200
833/834 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.8381 
Epoch 46: val_accuracy did not improve from 0.80700
834/834 [==============================] - 35s 42ms/step - loss: 0.4505 - accuracy: 0.8381 - val_loss: 0.5819 - val_accuracy: 0.8042 - lr: 0.0010
Epoch 47/200
833/834 [============================>.] - ETA: 0s - loss: 0.4449 - accuracy: 0.8403 
Epoch 47: val_accuracy did not improve from 0.80700
834/834 [==============================] - 34s 41ms/step - loss: 0.4448 - accuracy: 0.8403 - val_loss: 0.5846 - val_accuracy: 0.8056 - lr: 0.0010
Epoch 48/200
834/834 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.8433 
Epoch 48: val_accuracy did not improve from 0.80700
834/834 [==============================] - 34s 41ms/step - loss: 0.4398 - accuracy: 0.8433 - val_loss: 0.5989 - val_accuracy: 0.8037 - lr: 0.0010
Epoch 49/200
834/834 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8451 
Epoch 49: val_accuracy improved from 0.80700 to 0.80770, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.4317 - accuracy: 0.8451 - val_loss: 0.5778 - val_accuracy: 0.8077 - lr: 0.0010
Epoch 50/200
834/834 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8477 
Epoch 50: val_accuracy did not improve from 0.80770
834/834 [==============================] - 34s 41ms/step - loss: 0.4241 - accuracy: 0.8477 - val_loss: 0.5803 - val_accuracy: 0.8026 - lr: 0.0010
Epoch 51/200
833/834 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8534 
Epoch 51: val_accuracy improved from 0.80770 to 0.81090, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.4119 - accuracy: 0.8533 - val_loss: 0.5880 - val_accuracy: 0.8109 - lr: 0.0010
Epoch 52/200
833/834 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8551 
Epoch 52: val_accuracy did not improve from 0.81090
834/834 [==============================] - 34s 41ms/step - loss: 0.4092 - accuracy: 0.8551 - val_loss: 0.5767 - val_accuracy: 0.8090 - lr: 0.0010
Epoch 53/200
834/834 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.8558 
Epoch 53: val_accuracy did not improve from 0.81090
834/834 [==============================] - 34s 40ms/step - loss: 0.4018 - accuracy: 0.8558 - val_loss: 0.6053 - val_accuracy: 0.8047 - lr: 0.0010
Epoch 54/200
833/834 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8578 
Epoch 54: val_accuracy did not improve from 0.81090
834/834 [==============================] - 35s 42ms/step - loss: 0.3972 - accuracy: 0.8577 - val_loss: 0.5955 - val_accuracy: 0.8041 - lr: 0.0010
Epoch 55/200
834/834 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8600 
Epoch 55: val_accuracy improved from 0.81090 to 0.81120, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.3900 - accuracy: 0.8600 - val_loss: 0.5772 - val_accuracy: 0.8112 - lr: 0.0010
Epoch 56/200
833/834 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8643 
Epoch 56: val_accuracy did not improve from 0.81120
834/834 [==============================] - 35s 42ms/step - loss: 0.3794 - accuracy: 0.8643 - val_loss: 0.5859 - val_accuracy: 0.8087 - lr: 0.0010
Epoch 57/200
833/834 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8655 
Epoch 57: val_accuracy did not improve from 0.81120
834/834 [==============================] - 34s 41ms/step - loss: 0.3730 - accuracy: 0.8655 - val_loss: 0.5859 - val_accuracy: 0.8105 - lr: 0.0010
Epoch 58/200
834/834 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8673 
Epoch 58: val_accuracy did not improve from 0.81120
834/834 [==============================] - 34s 41ms/step - loss: 0.3648 - accuracy: 0.8673 - val_loss: 0.5762 - val_accuracy: 0.8086 - lr: 0.0010
Epoch 59/200
833/834 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8738 
Epoch 59: val_accuracy improved from 0.81120 to 0.81570, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 34s 41ms/step - loss: 0.3523 - accuracy: 0.8737 - val_loss: 0.5732 - val_accuracy: 0.8157 - lr: 0.0010
Epoch 60/200
833/834 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8741 
Epoch 60: val_accuracy did not improve from 0.81570
834/834 [==============================] - 34s 41ms/step - loss: 0.3538 - accuracy: 0.8741 - val_loss: 0.6156 - val_accuracy: 0.8024 - lr: 0.0010
Epoch 61/200
834/834 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8736 
Epoch 61: val_accuracy did not improve from 0.81570
834/834 [==============================] - 35s 42ms/step - loss: 0.3529 - accuracy: 0.8736 - val_loss: 0.5882 - val_accuracy: 0.8108 - lr: 0.0010
Epoch 62/200
834/834 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8759 
Epoch 62: val_accuracy did not improve from 0.81570
834/834 [==============================] - 35s 42ms/step - loss: 0.3438 - accuracy: 0.8759 - val_loss: 0.5879 - val_accuracy: 0.8125 - lr: 0.0010
Epoch 63/200
833/834 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8798  
Epoch 63: val_accuracy did not improve from 0.81570
834/834 [==============================] - 36s 43ms/step - loss: 0.3360 - accuracy: 0.8798 - val_loss: 0.5975 - val_accuracy: 0.8144 - lr: 0.0010
Epoch 64/200
834/834 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.8798 
Epoch 64: val_accuracy did not improve from 0.81570
834/834 [==============================] - 36s 43ms/step - loss: 0.3354 - accuracy: 0.8798 - val_loss: 0.6087 - val_accuracy: 0.8112 - lr: 0.0010
Epoch 65/200
833/834 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8817 
Epoch 65: val_accuracy improved from 0.81570 to 0.81610, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.3315 - accuracy: 0.8817 - val_loss: 0.5825 - val_accuracy: 0.8161 - lr: 0.0010
Epoch 66/200
834/834 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8854 
Epoch 66: val_accuracy did not improve from 0.81610
834/834 [==============================] - 36s 43ms/step - loss: 0.3200 - accuracy: 0.8854 - val_loss: 0.6190 - val_accuracy: 0.8090 - lr: 0.0010
Epoch 67/200
834/834 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8871 
Epoch 67: val_accuracy did not improve from 0.81610
834/834 [==============================] - 36s 43ms/step - loss: 0.3143 - accuracy: 0.8871 - val_loss: 0.6269 - val_accuracy: 0.8046 - lr: 0.0010
Epoch 68/200
834/834 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.8887 
Epoch 68: val_accuracy improved from 0.81610 to 0.81790, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.3085 - accuracy: 0.8887 - val_loss: 0.5899 - val_accuracy: 0.8179 - lr: 0.0010
Epoch 69/200
833/834 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8886 
Epoch 69: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 43ms/step - loss: 0.3039 - accuracy: 0.8886 - val_loss: 0.5936 - val_accuracy: 0.8148 - lr: 0.0010
Epoch 70/200
833/834 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8912 
Epoch 70: val_accuracy did not improve from 0.81790
834/834 [==============================] - 35s 43ms/step - loss: 0.3009 - accuracy: 0.8911 - val_loss: 0.5953 - val_accuracy: 0.8120 - lr: 0.0010
Epoch 71/200
833/834 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8958 
Epoch 71: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 44ms/step - loss: 0.2934 - accuracy: 0.8958 - val_loss: 0.6393 - val_accuracy: 0.8104 - lr: 0.0010
Epoch 72/200
834/834 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8952 
Epoch 72: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 43ms/step - loss: 0.2948 - accuracy: 0.8952 - val_loss: 0.6106 - val_accuracy: 0.8132 - lr: 0.0010
Epoch 73/200
834/834 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8978 
Epoch 73: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 43ms/step - loss: 0.2841 - accuracy: 0.8978 - val_loss: 0.6044 - val_accuracy: 0.8137 - lr: 0.0010
Epoch 74/200
834/834 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.8981 
Epoch 74: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 43ms/step - loss: 0.2821 - accuracy: 0.8981 - val_loss: 0.6056 - val_accuracy: 0.8149 - lr: 0.0010
Epoch 75/200
833/834 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.8997 
Epoch 75: val_accuracy did not improve from 0.81790
834/834 [==============================] - 36s 43ms/step - loss: 0.2761 - accuracy: 0.8996 - val_loss: 0.6084 - val_accuracy: 0.8155 - lr: 0.0010
Epoch 76/200
833/834 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.9013 
Epoch 76: val_accuracy improved from 0.81790 to 0.81930, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.2738 - accuracy: 0.9013 - val_loss: 0.6142 - val_accuracy: 0.8193 - lr: 0.0010
Epoch 77/200
833/834 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9022 
Epoch 77: val_accuracy improved from 0.81930 to 0.82100, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 43ms/step - loss: 0.2689 - accuracy: 0.9021 - val_loss: 0.6034 - val_accuracy: 0.8210 - lr: 0.0010
Epoch 78/200
834/834 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9047 
Epoch 78: val_accuracy did not improve from 0.82100
834/834 [==============================] - 36s 43ms/step - loss: 0.2654 - accuracy: 0.9047 - val_loss: 0.6271 - val_accuracy: 0.8177 - lr: 0.0010
Epoch 79/200
833/834 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.9054 
Epoch 79: val_accuracy did not improve from 0.82100
834/834 [==============================] - 36s 43ms/step - loss: 0.2597 - accuracy: 0.9054 - val_loss: 0.6208 - val_accuracy: 0.8186 - lr: 0.0010
Epoch 80/200
834/834 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9091 
Epoch 80: val_accuracy improved from 0.82100 to 0.82300, saving model to artifacts/cnn_model.h5
834/834 [==============================] - 36s 44ms/step - loss: 0.2519 - accuracy: 0.9091 - val_loss: 0.5996 - val_accuracy: 0.8230 - lr: 0.0010
Epoch 81/200
834/834 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9077 
Epoch 81: val_accuracy did not improve from 0.82300
834/834 [==============================] - 36s 43ms/step - loss: 0.2545 - accuracy: 0.9077 - val_loss: 0.6019 - val_accuracy: 0.8222 - lr: 0.0010
Epoch 82/200
834/834 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9085 
Epoch 82: val_accuracy did not improve from 0.82300
834/834 [==============================] - 35s 43ms/step - loss: 0.2482 - accuracy: 0.9085 - val_loss: 0.6369 - val_accuracy: 0.8169 - lr: 0.0010
Epoch 83/200
833/834 [============================>.] - ETA: 0s - loss: 0.2447 - accuracy: 0.9112 
Epoch 83: val_accuracy did not improve from 0.82300
834/834 [==============================] - 35s 43ms/step - loss: 0.2446 - accuracy: 0.9113 - val_loss: 0.6085 - val_accuracy: 0.8207 - lr: 0.0010
Epoch 84/200
833/834 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.9131 
Epoch 84: val_accuracy did not improve from 0.82300
834/834 [==============================] - 36s 43ms/step - loss: 0.2414 - accuracy: 0.9131 - val_loss: 0.6096 - val_accuracy: 0.8174 - lr: 0.0010
Epoch 85/200
833/834 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9127 
Epoch 85: val_accuracy did not improve from 0.82300
834/834 [==============================] - 36s 43ms/step - loss: 0.2395 - accuracy: 0.9127 - val_loss: 0.6217 - val_accuracy: 0.8206 - lr: 0.0010
Epoch 86/200
834/834 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9149 
Epoch 86: val_accuracy did not improve from 0.82300
834/834 [==============================] - 36s 43ms/step - loss: 0.2349 - accuracy: 0.9149 - val_loss: 0.6169 - val_accuracy: 0.8210 - lr: 0.0010
Epoch 87/200
834/834 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9192 
Epoch 87: val_accuracy did not improve from 0.82300
834/834 [==============================] - 36s 43ms/step - loss: 0.2274 - accuracy: 0.9192 - val_loss: 0.6329 - val_accuracy: 0.8179 - lr: 0.0010
Epoch 88/200
833/834 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9181 
Epoch 88: val_accuracy did not improve from 0.82300

Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
834/834 [==============================] - 37s 44ms/step - loss: 0.2265 - accuracy: 0.9181 - val_loss: 0.6236 - val_accuracy: 0.8206 - lr: 0.0010
Epoch 88: early stopping
(tf) dhvanishah@Dhvanis-MBP vgg3 % python predict.py       
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 32, 32, 32)        896       
                                                                 
 conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      
                                                                 
 max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         
 D)                                                              
                                                                 
 dropout (Dropout)           (None, 16, 16, 32)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     
                                                                 
 conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         
 g2D)                                                            
                                                                 
 dropout_1 (Dropout)         (None, 8, 8, 64)          0         
                                                                 
 conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     
                                                                 
 conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         
 g2D)                                                            
                                                                 
 dropout_2 (Dropout)         (None, 4, 4, 128)         0         
                                                                 
 flatten (Flatten)           (None, 2048)              0         
                                                                 
 dense (Dense)               (None, 128)               262272    
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 550570 (2.10 MB)
Trainable params: 550570 (2.10 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Train: X=(50000, 32, 32, 3), y=(50000, 1)
Test: X=(10000, 32, 32, 3), y=(10000, 1)
/Users/dhvanishah/opt/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
313/313 [==============================] - 13s 42ms/step
Inference time per CPU thread per sample: 1.3383677005767822 ms
Inference time per CPU thread for 10000 test samples : 13.383677005767822 s
[[841   7  30  18  14   4  11   7  46  22]
 [  9 891   2   3   2   1   7   1  29  55]
 [ 48   3 721  33  63  53  48  16  11   4]
 [ 13   5  52 621  56 159  61  20   6   7]
 [  7   2  39  46 816  16  40  22   7   5]
 [  5   1  26 128  40 758  24  16   2   0]
 [  1   2  28  37  19  18 889   1   4   1]
 [  8   1  16  28  49  60  10 822   1   5]
 [ 32   8   8   9   2   4   4   1 918  14]
 [ 22  47   1  11   1   2   7   5  19 885]]

Accuracy: 0.82

Micro Precision: 0.82
Micro Recall: 0.82
Micro F1-score: 0.82

Macro Precision: 0.82
Macro Recall: 0.82
Macro F1-score: 0.82

Weighted Precision: 0.82
Weighted Recall: 0.82
Weighted F1-score: 0.82

Classification Report

              precision    recall  f1-score   support

    airplane       0.85      0.84      0.85      1000
  automobile       0.92      0.89      0.91      1000
        bird       0.78      0.72      0.75      1000
         cat       0.66      0.62      0.64      1000
        deer       0.77      0.82      0.79      1000
         dog       0.71      0.76      0.73      1000
        frog       0.81      0.89      0.85      1000
       horse       0.90      0.82      0.86      1000
        ship       0.88      0.92      0.90      1000
       truck       0.89      0.89      0.89      1000

    accuracy                           0.82     10000
   macro avg       0.82      0.82      0.82     10000
weighted avg       0.82      0.82      0.82     10000

(tf) dhvanishah@Dhvanis-MBP vgg3 %    